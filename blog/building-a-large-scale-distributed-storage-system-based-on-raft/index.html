<!DOCTYPE html>
<html lang="en"><head>
<meta name="robots" content="noindex"><meta charset="utf-8"/><meta content="IE=edge" http-equiv="X-UA-Compatible"/><meta content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no" name="viewport"/>
<script>
window.ga =
    window.ga ||
    function() {
        ;(ga.q = ga.q || []).push(arguments)
    }
ga.l = +new Date()
ga('create', 'UA-99991864-4', 'auto')
ga('send', 'pageview')
</script>
<script async="" src="https://download.pingcap.com/js/ga-analytics.js"></script>
<link href="https://download.pingcap.com/style/github-markdown.css" rel="stylesheet"/>
<link href="/css/doc.css" rel="stylesheet"/>
<title>Building a Large-scale Distributed Storage System Based on Raft |Â TiDB</title><link href="/images/favicon.ico" rel="icon" type="image/x-icon"/>
<link href="/images/favicon.ico" rel="shortcut icon" type="image/x-icon"/>
<link href="/images/apple-touch-icon.png" rel="apple-touch-icon"/>
<meta content="This post introduces the PingCAP team's firsthand experience in designing a large-scale distributed storage system based on the Raft consensus algorithm." name="description"/>
<meta content="noodp" name="robots"/>
<meta content="TiDB, MySQL, HTAP, Open Source, Cloud-Native, NewSQL, Aurora Alternative" name="keywords"/>
<meta content="en_US" property="og:locale"/>
<meta content="website" property="og:type"/>
<meta content="Building a Large-scale Distributed Storage System Based on Raft | TiDB" property="og:title"/>
<meta content="This post introduces the PingCAP team's firsthand experience in designing a large-scale distributed storage system based on the Raft consensus algorithm." property="og:description"/>
<meta content="https://pingcap.com/blog/building-a-large-scale-distributed-storage-system-based-on-raft/" property="og:url"/>
<meta content="MySQL at Scale. No more manual sharding" property="og:site_name"/>
<meta content="/images/pingcap-opengraph.jpg" property="og:image"/>
<meta content="/images/pingcap-opengraph.jpg" property="og:image:secure_url"/><meta content="summary" name="twitter:card"/><meta content="This post introduces the PingCAP team's firsthand experience in designing a large-scale distributed storage system based on the Raft consensus algorithm." name="twitter:description"/>
<meta content="Building a Large-scale Distributed Storage System Based on Raft | TiDB" name="twitter:title"/>
<meta content="@pingcap" name="twitter:site"/>
<meta content="https://download.pingcap.com/images/pingcap-opengraph.jpg" name="twitter:image"/><meta content="@pingcap" name="twitter:creator"/>
<script type="application/ld+json">
{
    "@context": "http:\/\/schema.org",
    "@type": "WebSite",
    "url": "https:\/\/pingcap.com\/",
    "name": "PingCAP",
    "potentialAction": {
        "@type": "SearchAction",
        "target": "https:\/\/pingcap.com\/?s={search_term_string}",
        "query-input": "required name=search_term_string"
    }
}
</script>
<script>
    (function(){
        var bp = document.createElement('script');
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>
<script async="" src="https://accounts.pingcap.com/static/pingcap_sso/js/track.beta.js" type="text/javascript"></script>
<link as="font" crossorigin="anonymous" href="https://download.pingcap.com/fonts/lato/lato-regular-webfont.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://download.pingcap.com/fonts/lato/lato-regular-webfont.woff" rel="preload" type="font/woff"/>
<link as="font" crossorigin="anonymous" href="https://download.pingcap.com/fonts/lato/lato-regular-webfont.ttf" rel="preload" type="font/ttf"/>
<link as="font" crossorigin="anonymous" href="https://download.pingcap.com/fonts/titilliumweb/titilliumweb-regular-webfont.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://download.pingcap.com/fonts/titilliumweb/titilliumweb-regular-webfont.woff" rel="preload" type="font/woff"/>
<link as="font" crossorigin="anonymous" href="https://download.pingcap.com/fonts/titilliumweb/titilliumweb-regular-webfont.ttf" rel="preload" type="font/ttf"/>
<link as="script" href="https://download.pingcap.com/js/jquery.min.js" rel="preload"/>
<link href="/css/main.css" rel="stylesheet"/>
<link href="https://download.pingcap.com/style/docsearch.min.css" rel="stylesheet"/>
<script type="text/javascript">
var trackOutboundLink = function(url) {
  var redirectTriggered = false
  ga('send', 'event', 'outbound', 'click', url, {
    hitCallback: function() {
      redirectTriggered = true
      document.location = url
    }
  })
  setTimeout(function() {
    if (!redirectTriggered) {
      document.location = url
    }
  }, 1500)
}

var trackViews = function(btn_type, event_category) {
  var event_label = btn_type
  var eventCategory = event_category
  ga('send', 'event', {
    'eventCategory': eventCategory,
    'eventAction': 'click',
    'eventLabel': event_label,
    'transport': 'beacon'
  });
}
</script>
<script>if (location.pathname === '/') {
        if (location.hostname === 'pingcap.github.io') {
            location.href = '/en/'
        }
    }</script></head> <body data-lang="en"><div id="page-content">
<header class="fixed-top" role="navigation">
<div class="container">
<a class="nav-brand" href="/en"><img alt="PingCAP" src="/images/pingcap-logo.png"/></a>
<div class="nav-wrapper">
<div class="navigation">
<ul>
<li><a class="a-en" href="https://en.pingcap.com">Cloud</a></li>
<li><a class="a-en" href="/tidb-academy">TiDB Academy</a></li>
<li class="docs-type-selector " id="docsTypeSelector">
<a class="header-doc-nav" href="#">Docs</a>
<div class="header-dropdown-menu" id="docsTypeSelectorItem">
<a class="header-dropdown-item" href="/docs/stable/">TiDB</a>
<a class="header-dropdown-item" href="/docs/tidb-in-kubernetes/stable/">TiDB in Kubernetes</a>
<a class="header-dropdown-item" href="/docs/tidb-data-migration/stable/">TiDB Data Migration (DM)</a>
</div>
</li>
<li><a class="a-en" href="/success-stories">Success
                    Stories</a></li>
<li class="sel"><a class="a-en" href="/blog">Blog</a></li>
<li><a class="link-download link-download-en" href="/download">Free Download</a></li>
</ul>
</div>
<div class="global-search">
<div class="search-wrapper">
<span class="icon-search"><svg fill="#fff" height="18" viewbox="0 0 67.125 67.125" width="18" xmlns="http://www.w3.org/2000/svg"><path d="M23.902 0C11.01 0 .523 10.488.523 23.38c0 12.891 10.487 23.379 23.379 23.379a23.258 23.258 0 0 0 14.471-5.037l24.816 24.817c.391.391.902.586 1.414.586s1.023-.195 1.414-.586a2 2 0 0 0 0-2.828L41.293 38.985c3.721-4.142 5.989-9.613 5.989-15.605C47.282 10.488 36.794 0 23.902 0zM4.523 23.38C4.523 12.694 13.216 4 23.902 4c10.687 0 19.38 8.694 19.38 19.38 0 5.396-2.221 10.279-5.791 13.795a1.959 1.959 0 0 0-.488.349 2.003 2.003 0 0 0-.271.343C33.31 40.9 28.825 42.76 23.903 42.76c-10.686-.001-19.38-8.695-19.38-19.38z"></path><path d="M24.075 8.591c-8.25 0-14.962 6.712-14.962 14.962a2 2 0 0 0 4 0c0-6.044 4.918-10.962 10.962-10.962a2 2 0 0 0 0-4z"></path></svg></span>
<input data-lang="en" data-type="" id="search-input" name="q" placeholder="Search TiDB Docs" type="search"/>
</div>
<script>
    const inputNode = document.getElementById("search-input")

    inputNode.addEventListener('keyup', ({key}) => {
        if (key === "Enter") {
            if ($('#search-input').data('lang') == 'cn') {
                lang = 'zh'
            }

            const query = $('#search-input').val()
            if (query) {
                url = "https://docs.pingcap.com/" + (lang == 'zh' ? 'zh/' : '') + "search/?lang=" + lang + "&type=tidb&version=v4.0&q=" + query
                window.location.href = url
            }
        }
    })
</script>
</div>
</div>
<div class="nav-btn nav-slider">
<i class="material-icons"><svg height="16" viewbox="0 0 459 459" width="16" xmlns="http://www.w3.org/2000/svg"><path d="M0 382.5h459v-51H0v51zM0 255h459v-51H0v51zM0 76.5v51h459v-51H0z" fill="#FFF"></path></svg></i>
</div>
</div>
</header>
<nav class="mobile-sidebar">
<div class="nav-header">
<div class="logo-wrap">
<a class="nav-brand" href="/en"><img alt="PingCAP" src="/images/pingcap-logo.png"/></a>
</div>
</div>
<ul class="ul-base">
<li><a href="https://en.pingcap.com/">Cloud</a></li>
<li><a class="tidb-academy" href="/tidb-academy">TiDB Academy</a></li>
<li class="docs-type-selector " id="docsTypeSelector">
<p class="header-doc-nav">Docs</p>
<div class="header-dropdown-menu" id="docsTypeSelectorItem">
<a class="header-dropdown-item" href="/docs/stable/">TiDB</a>
<a class="header-dropdown-item" href="/docs/tidb-in-kubernetes/stable/">TiDB in Kubernetes</a>
<a class="header-dropdown-item" href="/docs/tidb-data-migration/stable/">TiDB Data Migration (DM)</a>
</div>
</li>
<li><a href="/success-stories">Success Stories</a></li>
<li class="sel"><a href="/blog">Blog</a></li>
<li><a class="link-download" href="/download">Free Download</a></li>
</ul>
<div class="nav-footer">
<div class="contact-list-wrap">
<div class="flex-list">
<h4 class="list-title"><strong>Contact</strong></h4>
<ul class="social ul-base">
<li><a class="twitter" href="https://twitter.com/PingCAP" target="_blank"><svg height="18" viewbox="0 0 612 612" width="18" xmlns="http://www.w3.org/2000/svg"><path d="M612 116.258a250.714 250.714 0 0 1-72.088 19.772c25.929-15.527 45.777-40.155 55.184-69.411-24.322 14.379-51.169 24.82-79.775 30.48-22.907-24.437-55.49-39.658-91.63-39.658-69.334 0-125.551 56.217-125.551 125.513 0 9.828 1.109 19.427 3.251 28.606-104.326-5.24-196.835-55.223-258.75-131.174-10.823 18.51-16.98 40.078-16.98 63.101 0 43.559 22.181 81.993 55.835 104.479a125.556 125.556 0 0 1-56.867-15.756v1.568c0 60.806 43.291 111.554 100.693 123.104-10.517 2.83-21.607 4.398-33.08 4.398-8.107 0-15.947-.803-23.634-2.333 15.985 49.907 62.336 86.199 117.253 87.194-42.947 33.654-97.099 53.655-155.916 53.655-10.134 0-20.116-.612-29.944-1.721 55.567 35.681 121.536 56.485 192.438 56.485 230.948 0 357.188-191.291 357.188-357.188l-.421-16.253c24.666-17.593 46.005-39.697 62.794-64.861z"></path></svg></a></li>
<li><a class="linkedin" href="https://www.linkedin.com/company/pingcap/" target="_blank"><svg height="18" viewbox="0 0 438.536 438.535" width="18" xmlns="http://www.w3.org/2000/svg"><path d="M5.424 145.895H99.64v282.932H5.424zM408.842 171.739c-19.791-21.604-45.967-32.408-78.512-32.408-11.991 0-22.891 1.475-32.695 4.427-9.801 2.95-18.079 7.089-24.838 12.419-6.755 5.33-12.135 10.278-16.129 14.844-3.798 4.337-7.512 9.389-11.136 15.104v-40.232h-93.935l.288 13.706c.193 9.139.288 37.307.288 84.508 0 47.205-.19 108.777-.572 184.722h93.931V270.942c0-9.705 1.041-17.412 3.139-23.127 4-9.712 10.037-17.843 18.131-24.407 8.093-6.572 18.13-9.855 30.125-9.855 16.364 0 28.407 5.662 36.117 16.987 7.707 11.324 11.561 26.98 11.561 46.966V428.82h93.931V266.664c-.007-41.688-9.897-73.328-29.694-94.925zM53.103 9.708c-15.796 0-28.595 4.619-38.4 13.848C4.899 32.787 0 44.441 0 58.529 0 72.42 4.758 84.034 14.275 93.358c9.514 9.325 22.078 13.99 37.685 13.99h.571c15.99 0 28.887-4.661 38.688-13.99 9.801-9.324 14.606-20.934 14.417-34.829-.19-14.087-5.047-25.742-14.561-34.973C81.562 14.323 68.9 9.708 53.103 9.708z"></path></svg></a></li>
<li><a class="reddit" href="https://www.reddit.com/r/TiDB/" target="_blank"><svg height="18" viewbox="0 0 279.748 279.748" width="18" xmlns="http://www.w3.org/2000/svg"><path d="M279.748 133.142c0-19.299-15.701-35-35-35-10.768 0-20.674 4.812-27.279 13.064-18.532-8.431-39.663-13.626-62.015-15.271l19.206-60.692 42.895 9.294c3.285 12.782 14.901 22.258 28.693 22.258 16.336 0 29.627-13.29 29.627-29.626 0-16.336-13.291-29.627-29.627-29.627-11.801 0-21.999 6.941-26.759 16.95l-49.497-10.725a10.002 10.002 0 0 0-11.651 6.756L134.636 95.43c-26.164.638-50.988 6.053-72.356 15.775-6.606-8.251-16.512-13.063-27.28-13.063-19.299 0-35 15.701-35 35 0 9.373 3.683 18.173 10.222 24.709-3.9 8.37-5.875 17.076-5.875 25.936 0 24.048 14.396 46.492 40.538 63.199 25.447 16.264 59.183 25.221 94.989 25.221 35.808 0 69.542-8.957 94.989-25.221 26.142-16.707 40.538-39.151 40.538-63.199 0-8.859-1.975-17.565-5.875-25.936 6.539-6.537 10.222-15.336 10.222-24.709zM15.369 145.139c-2.212-3.59-3.369-7.688-3.369-11.997 0-12.682 10.317-23 23-23 5.444 0 10.558 1.851 14.649 5.258-14.622 8.302-26.132 18.289-34.28 29.739zm52.671 20.266c0-13.785 11.215-25 25-25s25 11.215 25 25-11.215 25-25 25-25-11.215-25-25zm123.119 57.054c-9.745 10.637-29.396 17.244-51.285 17.244-21.888 0-41.539-6.607-51.284-17.244a9.937 9.937 0 0 1-2.617-7.192 9.933 9.933 0 0 1 3.235-6.937 9.974 9.974 0 0 1 6.754-2.627c2.797 0 5.484 1.183 7.373 3.244 5.803 6.333 20.827 10.756 36.539 10.756s30.737-4.423 36.539-10.756a10.022 10.022 0 0 1 7.374-3.244c2.508 0 4.906.933 6.755 2.627a9.928 9.928 0 0 1 3.234 6.937 9.933 9.933 0 0 1-2.617 7.192zm-4.451-32.054c-13.785 0-25-11.215-25-25s11.215-25 25-25 25 11.215 25 25-11.215 25-25 25zm77.671-45.266c-8.147-11.45-19.657-21.436-34.28-29.739 4.092-3.408 9.205-5.258 14.649-5.258 12.683 0 23 10.318 23 23 0 4.309-1.157 8.407-3.369 11.997z"></path></svg></a></li>
<li><a class="google-plus" href="https://groups.google.com/forum/#!forum/tidb-user" target="_blank"><svg height="18" viewbox="0 0 491.858 491.858" width="18" xmlns="http://www.w3.org/2000/svg"><path d="M377.472 224.957H201.319v58.718H308.79c-16.032 51.048-63.714 88.077-120.055 88.077-69.492 0-125.823-56.335-125.823-125.824 0-69.492 56.333-125.823 125.823-125.823 34.994 0 66.645 14.289 89.452 37.346l42.622-46.328c-34.04-33.355-80.65-53.929-132.074-53.929C84.5 57.193 0 141.693 0 245.928s84.5 188.737 188.736 188.737c91.307 0 171.248-64.844 188.737-150.989v-58.718l-.001-.001zM491.858 224.857h-36.031v-36.031h-30.886v36.031H388.91v30.883h36.031v36.032h30.886V255.74h36.031z"></path></svg></a></li>
<li><a class="stack-overflow" href="https://stackoverflow.com/questions/tagged/tidb" target="_blank"><svg height="18" viewbox="0 0 547.597 547.597" width="18" xmlns="http://www.w3.org/2000/svg"><path d="M140.81 475.111h.024l189.91-.269c8.434-.013 15.3-6.886 15.3-15.318v-21.298c0-8.428-6.854-15.288-15.3-15.288l-189.91.264c-8.433.012-15.3 6.885-15.3 15.318v21.31c.001 8.427 6.855 15.281 15.276 15.281z"></path><path d="M58.667 517.854c.073 29.26.073 29.449 3.072 29.449h.055l10.612.294h.086s85.814 0 171.629-.036c42.914-.019 85.821-.05 118-.092 16.09-.019 29.505-.043 38.887-.074 17.803-.055 17.803-.055 17.803-3.072l.3-10.697V333.299c0-8.434-6.866-15.3-15.3-15.3h-11.909c-8.434 0-15.3 6.866-15.3 15.3V496.22c0 5.062-4.119 9.18-9.181 9.18H110.51c-5.061 0-9.18-4.118-9.18-9.18V333.299c0-8.434-6.867-15.3-15.3-15.3H73.82c-8.433 0-15.3 6.86-15.3 15.3 0 23.342.012 76.084.055 122.981.019 23.447.05 45.442.092 61.574z"></path><path d="M142.322 397.399l189.402 17.467c.478.043.948.062 1.413.062 7.956 0 14.468-5.985 15.159-13.917l1.83-21.096c.729-8.396-5.508-15.851-13.898-16.628l-189.102-17.461c-8.593-.795-15.869 5.441-16.653 13.825l-1.97 21.108a15.172 15.172 0 0 0 3.452 11.181 15.19 15.19 0 0 0 10.367 5.459zM437.636 208.849a15.253 15.253 0 0 0 17.694 12.443l21.065-3.678c8.311-1.451 13.898-9.395 12.454-17.706l-32.504-187.23c-1.42-8.207-9.21-13.917-17.692-12.448l-21.065 3.678c-8.311 1.451-13.898 9.395-12.454 17.705l32.502 187.236zM190.333 194.375l163.6 96.708a15.28 15.28 0 0 0 7.778 2.136c5.393 0 10.447-2.876 13.183-7.509l10.876-18.36c2.08-3.519 2.674-7.631 1.658-11.591a15.18 15.18 0 0 0-7.032-9.358l-163.6-96.714c-7.014-4.149-16.836-1.597-20.967 5.374l-10.869 18.36a15.2 15.2 0 0 0-1.658 11.591 15.21 15.21 0 0 0 7.031 9.363zM387.525 240.501a15.276 15.276 0 0 0 12.626 6.659c3.091 0 6.077-.924 8.635-2.681l17.405-11.934c6.953-4.768 8.752-14.314 4.015-21.292L323.29 54.104c-4.584-6.732-14.498-8.623-21.236-3.984l-17.737 12.21c-6.945 4.779-8.727 14.327-3.971 21.291l107.179 156.88zM154.482 302.319l183.465 49.156c1.298.349 2.632.526 3.966.526 6.903 0 12.98-4.67 14.762-11.347l5.508-20.624c2.179-8.152-2.681-16.555-10.826-18.74l-183.465-49.162c-8.017-2.148-16.604 2.852-18.728 10.826l-5.508 20.63c-2.179 8.142 2.68 16.551 10.826 18.735z"></path></svg></a></li>
</ul>
</div>
<div class="subscribe">
<a href="https://share.hsforms.com/1e2W03wLJQQKPd1d9rCbj_Q2npzm"><button class="btn btn-subscribe f-tc" style="margin-left: 0.75rem;">Subscribe to Blog</button></a>
</div>
</div>
<div class="container">
<a class="btn-lang" href="/blog-cn" id="lang">ä¸­æ</a>
</div>
</div>
</nav>
<div class="blog">
<div class="blogArticle__container">
<div class="archive"><div class="article-nav">
<a href="/blog/#Engineering">BLOG HOME</a> <span> &gt; </span>Engineering</div><div class="content markdown-body">
<h1 class="title">Building a Large-scale Distributed Storage System Based on Raft</h1>
<ul class="blog-post-meta">
<li class="meta-item">
<img alt="Date icon" src="/images/svgs/icon-date.svg"/>Wed, Nov 20, 2019</li>
<li class="meta-item">
<img alt="Pen icon" src="/images/svgs/icon-writer.svg"/>Edward Huang</li>
</ul>
<div class="blog-text"><p>In recent years, building <strong>a large-scale distributed storage system</strong> has become a hot topic. Distributed consensus algorithms like <a href="https://en.wikipedia.org/wiki/Paxos_(computer_science)">Paxos</a> and <a href="https://raft.github.io/">Raft</a> are the focus of many technical articles. But those articles tend to be introductory, describing the basics of the algorithm and log replication. They seldom cover how to build a large-scale distributed storage system based on the distributed consensus algorithm.</p>
<p>Since April 2015, we <a href="https://pingcap.com/">PingCAP</a> have been building <a href="https://github.com/tikv/tikv">TiKV</a>, a large-scale open source distributed database based on Raft. It's the core storage component of <a href="https://github.com/pingcap/tidb">TiDB</a>, an open source distributed NewSQL database that supports Hybrid Transactional and Analytical Processing (HTAP) workloads. Earlier in 2019, we conducted an official Jepsen test on TiDB, and <a href="https://jepsen.io/analyses/tidb-2.1.7">the Jepsen test report</a> was published in June 2019. In July the same year, we announced that <a href="https://pingcap.com/blog/tidb-3.0-announcement/">TiDB 3.0 reached general availability</a>, delivering stability at scale and performance boost.</p>
<p>In this article, I'd like to share some of our firsthand experience in <strong>designing a large-scale distributed storage system</strong> based on the <strong>Raft consensus algorithm</strong>.</p>
<h2 id="scaling-a-distributed-storage-system">Scaling a distributed storage system</h2>
<p>The first thing I want to talk about is scaling. The core of a distributed storage system is nothing more than two points: one is the sharding strategy, and the other is metadata storage. Keeping applications transparent and consistent in the sharding process is crucial to a storage system with elastic scalability.</p>
<p>If a storage system only has a static data sharding strategy, it is hard to elastically scale with application transparency. Such systems include MySQL static routing middleware like <a href="https://github.com/alibaba/cobar">Cobar</a>, Redis middleware like <a href="https://github.com/twitter/twemproxy/">Twemproxy</a>, and so on. All these systems are difficult to scale seamlessly.</p>
<p>Before moving on to elastic scalability, I'd like to talk about several sharding strategies.</p>
<h3 id="sharding-strategies-for-distributed-databases">Sharding strategies for distributed databases</h3>
<p>Sharding is a database partitioning strategy that splits your datasets into smaller parts and stores them in different physical nodes. The unit for data movement and balance is a sharding unit. Each physical node in the cluster stores several sharding units.</p>
<p>Two commonly-used sharding strategies are range-based sharding and hash-based sharding. The choice of the sharding strategy changes according to different types of systems. A typical example is the data distribution of a Hadoop Distributed File System (HDFS) DataNode, shown in Figure 1 (source: <a href="http://energystudy.synergylabs.org/courses/15-440-Fall2017/lectures/16-gfs_hdfs_spanner.pdf">Distributed Systems: GFS/HDFS/Spanner</a>).</p>
<p><img alt="Data distribution of HDFS DataNode" class="lazy" data-original="https://download.pingcap.com/images/blog/data-distribution-of-hdfs-dataNode.png" src="/images/svgs/loader-spinner.svg"/></p>
<div class="caption-center"> Figure 1. Data distribution of HDFS DataNode </div>
<h4 id="range-based-sharding">Range-based sharding</h4>
<p>Range-based sharding assumes that all keys in the database system can be put in order, and it takes a continuous section of keys as a sharding unit.</p>
<p>It's very common to sort keys in order. HBase keys are sorted in byte order, while MySQL keys are sorted in auto-increment ID order. For some storage engines, the order is natural. In the case of both log-structured merge-tree (LSM-Tree) and B-Tree, keys are naturally in order.</p>
<p><img alt="Range-based sharding for data partitioning" class="lazy" data-original="https://download.pingcap.com/images/blog/range-based-sharding-for-data-partitioning.png" src="/images/svgs/loader-spinner.svg"/></p>
<div class="caption-center"> Figure 2. Range-based sharding for data partitioning </div>
<p>In Figure 2 (source: <a href="https://docs.mongodb.com/manual/core/ranged-sharding/">MongoDB uses range-based sharding to partition data</a>), the key space is divided into (minKey, maxKey). Each sharding unit (chunk) is a section of continuous keys. The advantage of range-based sharding is that the adjacent data has a high probability of being together (such as the data with a common prefix), which can well support operations like <code>range scan</code>. For example, HBase Region is a typical range-based sharding strategy.</p>
<p>However, range-based sharding is not friendly to sequential writes with heavy workloads. For example, in the time series type of write load, the write hotspot is always in the last Region. This occurs because the log key is generally related to the timestamp, and the time is monotonically increasing. But relational databases often need to execute <code>table scan</code> (or <code>index scan</code>), and the common choice is range-based sharding.</p>
<h4 id="hash-based-sharding">Hash-based sharding</h4>
<p>Hash-based sharding processes keys using a hash function and then uses the results to get the sharding ID, as shown in Figure 3 (source: <a href="https://docs.mongodb.com/manual/core/hashed-sharding/">MongoDB uses hash-based sharding to partition data</a>).</p>
<p>Contrary to range-based sharding, where all keys can be put in order, hash-based sharding has the advantage that keys are distributed almost randomly, so the distribution is even. As a result, it is more friendly to systems with heavy write workloads and read workloads that are almost all random. This is because the write pressure can be evenly distributed in the cluster. But apparently, operations like <code>range scan</code> are almost impossible.</p>
<p><img alt="Hash-based sharding for data partitioning" class="lazy" data-original="https://download.pingcap.com/images/blog/hash-based-sharding-for-data-partitioning.png" src="/images/svgs/loader-spinner.svg"/></p>
<div class="caption-center"> Figure 3. Hash-based sharding for data partitioning </div>
<p>Some typical examples of hash-based sharding are <a href="https://docs.datastax.com/en/archived/cassandra/2.1/cassandra/architecture/architectureDataDistributeHashing_c.html">Cassandra Consistent hashing</a>, presharding of Redis Cluster and <a href="https://github.com/CodisLabs/codis">Codis</a>, and <a href="https://github.com/twitter/twemproxy/blob/master/README.md#features">Twemproxy consistent hashing</a>.</p>
<h4 id="hash-range-combination-sharding">Hash-Range combination sharding</h4>
<p>Note that hash-based and range-based sharding strategies are not isolated. Instead, you can flexibly combine them. For example, you can establish a multi-level sharding strategy, which uses hash in the uppermost layer, while in each hash-based sharding unit, data is stored in order.</p>
<h3 id="implementing-elastic-scalability-for-a-storage-system">Implementing elastic scalability for a storage system</h3>
<p>When it comes to elastic scalability, it's easy to implement for a system using range-based sharding: simply split the Region. Assuming that you have a Range Region [1, 100), you only need to choose a split point, such as 50. Then this Region is split into [1, 50) and [50, 100). After that, move the two Regions into two different machines, and the load is balanced.</p>
<p>Range-based sharding may bring read and write hotspots, but these hotspots can be eliminated by splitting and moving. Splitting and moving hotspots are lagging behind the hash-based sharding. But overall, for relational databases, range-based sharding is a good choice.</p>
<p>In contrast, implementing elastic scalability for a system using hash-based sharding is quite costly. The reason is obvious. Assume that the current system has three nodes, and you add a new physical node. In the hash model, ânâ changes from 3 to 4, which can cause a large system jitter. Although you can use a consistent hashing algorithm like <a href="https://github.com/RJ/ketama">Ketama</a> to reduce the system jitter as much as possible, it's hard to totally avoid it. This is because after a hash function is applied, data is randomly distributed, and adjusting the hash algorithm will certainly change the distribution rule for most data.</p>
<h3 id="implementing-data-consistency-and-high-availability">Implementing data consistency and high availability</h3>
<p>We chose range-based sharding for TiKV. After choosing an appropriate sharding strategy, we need to combine it with a high-availability replication solution. Different replication solutions can achieve different levels of availability and consistency.</p>
<p>Many middleware solutions simply implement a sharding strategy but without specifying the data replication solution on each shard. Examples include the Redis middleware <a href="https://github.com/twitter/twemproxy">twemproxy</a> and <a href="https://github.com/CodisLabs/codis">Codis</a>, and the MySQL middleware <a href="https://github.com/alibaba/cobar">Cobar</a>. These middleware solutions only implement routing in the middle layer, without considering the replication solution on each storage node in the bottom layer.</p>
<p>However, this replication solution matters a lot for a large-scale storage system. Generally, the number of shards in a system that supports elastic scalability changes, and so does the distribution of these shards. The system automatically balances the load, scaling out or in. If there is a large amount of data and a large number of shards, it's almost impossible to manually maintain the primary-secondary relationship, recover from failures, and so on. So it's very important to choose a highly-automated, high-availability solution.</p>
<p>In TiKV, each range shard is called a Region. Because we need to support scanning and the stored data generally has a relational table schema, we want the data of the same table to be as close as possible. Each Region in TiKV uses the Raft algorithm to ensure data security and high availability on multiple physical nodes.</p>
<p><img alt="Raft group in distributed database TiKV" class="lazy" data-original="https://download.pingcap.com/images/blog/raft-group-in-distributed-database-tikv.png" src="/images/svgs/loader-spinner.svg"/></p>
<div class="caption-center"> Figure 4. Raft group in distributed database TiKV </div>
<p>Several open source Raft implementations, including <a href="https://coreos.com/etcd/">etcd</a>, <a href="https://github.com/logcabin/logcabin">LogCabin</a>, <a href="https://github.com/pingcap/raft-rs">raft-rs</a> and <a href="https://www.consul.io/">Consul</a>, are just implementations of a single Raft group, which cannot be used to store a large amount of data. So the major use case for these implementations is configuration management. After all, the more participating nodes in a single Raft group, the worse the performance. If physical nodes cannot be added horizontally, the system has no way to scale.</p>
<p>If you use multiple Raft groups, which can be combined with the sharding strategy mentioned above, it seems that the implementation of horizontal scalability is very simple. Taking the replicas of each shard as a Raft group is the basis for TiKV to store massive data. However, it is much more complex to manage multiple, dynamically-split Raft groups than a single Raft group. As far as I know, TiKV is currently one of only a few open source projects that implement multiple Raft groups.</p>
<p>TiKV divides data into Regions according to the key range. When a Region becomes too large (the current limit is 96 MB), it splits into two new ones. This splitting happens on all physical nodes where the Region is located. The newly-generated replicas of the Region constitute a new Raft group.</p>
<p>Then here comes two questions:</p>
<ul>
<li>How do we ensure that the split operation is securely executed on each replica of this Region?</li>
<li>How do we guarantee application transparency?</li>
</ul>
<div class="trackable-btns">
<a href="/download" onclick="trackViews('Building a Large-scale Distributed Storage System Based on Raft', 'download-tidb-btn-middle')"><button>Download TiDB</button></a>
<a href="https://share.hsforms.com/1e2W03wLJQQKPd1d9rCbj_Q2npzm" onclick="trackViews('Building a Large-scale Distributed Storage System Based on Raft', 'subscribe-blog-btn-middle')"><button>Subscribe to Blog</button></a>
</div>
<p><strong>Question #1: How do we ensure the secure execution of the split operation on each Region replica?</strong></p>
<p>The solution is relatively easy. You can use the following approach, which is exactly what the Raft algorithm does:</p>
<ol>
<li>
<p>Take the split Region operation as a Raft log.</p>
</li>
<li>
<p>Let this log go through the Raft state machine. When the log is successfully applied, the operation is safely replicated.</p>
</li>
<li>
<p>Verify that the splitting log operation is accepted.</p>
</li>
<li>
<p>Let the new Region go through the Raft election process. As an alternative, you can use the original leader and let the other nodes where this new Region is located send heartbeats directly.</p>
</li>
</ol>
<p>The split process is coupled with network isolation, which can lead to very complicated cases. For example, assume that there are two nodes named A and B, and the Region leader is on node A:</p>
<ol>
<li>
<p>The leader initiates a Region split request: Region 1 [a, d) â the new Region 1 [a, b) + Region 2 [b, d). Node A first sends the heartbeat of Region 2 to node B. Node A also sends a snapshot of Region 2 to node B because there hasn't been any Region 2 information on node B.</p>
</li>
<li>
<p>At this time, Region 2 is split into the new Region 2 [b, c) and Region 3 [c, d). So the snapshot that node A sends to node B is the latest snapshot of Region 2 [b, c).</p>
</li>
<li>
<p>Now the split log of Region 1 has arrived at node B and the old Region 1 on node B has also split into Region 1 [a, b) and Region 2 [b, d).</p>
</li>
<li>
<p>Then the latest snapshot of Region 2 [b, c) arrives at node B. After the new Region 2 is applied, it must be guaranteed that the [c, d) data no longer exists on Region 2 at node B.</p>
</li>
</ol>
<p><strong>Question #2: How do we guarantee application transparency?</strong></p>
<p>Raft does a better job of transparency than Paxos. Specifically, Raft provides a clear configuration change process to make sure nodes can be securely and dynamically added or removed in a Raft group. With this algorithm, the rebalance process can be summarized as follows:</p>
<ol>
<li>Add a replica to a Region.</li>
<li>Transfer the leadership.</li>
<li>Remove the local replica.</li>
</ol>
<p>These steps are the standard Raft configuration change process. In TiKV, the implementation is a little bit different:</p>
<ol>
<li>
<p>The <code>conf change</code> operation is only executed after the <code>conf change</code> log is applied.</p>
</li>
<li>
<p>To avoid a disjoint majority, a Region group can only handle one conf change operation each time. This has been mentioned in <a href="https://www.google.com/url?q=https://github.com/ongardie/dissertation/blob/master/online-trim.pdf?raw%3Dtrue&amp;sa=D&amp;ust=1562069112724000&amp;usg=AFQjCNFJ0gqDMqK93RKVqVk5BeOK1NHCFw">Diego Ongaro's paper âConsensus: Bridging Theory and Practiceâ</a> (2014).</p>
</li>
</ol>
<p>The process in TiKV can guarantee correctness and is also relatively simple to implement.</p>
<p>In addition, to implement transparency at the application layer, it also requires collaboration with the client and the metadata management module. After all, when a Region leader is transferred away, the client's read and write requests to this Region are sent to the new leader node.</p>
<p>Note: In this context, âthe clientâ refers to the TiKV software development kit (SDK) client.</p>
<p>When a client reads or writes data, it uses the following process:</p>
<ol>
<li>
<p>The client caches a routing table of data to the local storage. The routing table is as follows:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">{startKey1, endKey1}   -&gt;     {Region1, NodeA}
{startKey2, endKey2}   -&gt;     {Region2, NodeB}
{startKey3, endKey3}   -&gt;     {Region3, NodeC}
â¦
</code></pre></div></li>
<li>
<p>According to the key accessed by the user, the client checks and obtains the following information:</p>
<ul>
<li>The interval the key belongs to</li>
<li>The Region the interval belongs to</li>
<li>The physical node the leader is on</li>
</ul>
</li>
<li>
<p>The client sends the request to the specific node directly. As I mentioned above, the leader might have been transferred to another node. Then the client might receive an error saying âRegion not leader.â</p>
</li>
<li>
<p>The client returns with the new leader.</p>
</li>
<li>
<p>The client updates its routing table cache.</p>
</li>
</ol>
<h2 id="scheduling-in-a-distributed-storage-system">Scheduling in a distributed storage system</h2>
<p>In this section, I'll discuss how scheduling is implemented in a large-scale distributed storage system.</p>
<h3 id="scheduling-using-placement-driver">Scheduling using Placement Driver</h3>
<p>The routing table is a very important module that stores all the Region distribution information. The routing table must guarantee accuracy and high availability.</p>
<p>In addition, to rebalance the data as described above, we need a scheduler with a global perspective. To dynamically adjust the distribution of Regions in each node, the scheduler needs to know which node has insufficient capacity, which node is more stressed, and which node has more Region leaders on it. This is because all nodes are almost stateless, and they cannot migrate the data autonomously. Instead, they must rely on the scheduler to initiate data migration (<code>raft conf change</code>).</p>
<p>You might have noticed that you can integrate the scheduler and the routing table into one module. <a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/65b514eda12d025585183a641b5a9e096a3c4be5.pdf">Google's Spanner database</a> uses this single-module approach and calls it the placement driver. We also use this name in TiKV, and call it PD for short. PD is mainly responsible for the two jobs mentioned above: the routing table and the scheduler.</p>
<h3 id="pd-being-completely-stateless-guarantees-routing-data-consistency">PD being completely stateless guarantees routing data consistency</h3>
<p>Google's Spanner paper does not describe the placement driver design in detail. However, it's certain that one core idea in designing a large-scale distributed storage system is to assume that any module can crash. It's very dangerous if the states of modules rely on each other. This is because once an instance crashes, the standby instance must start immediately, but the state of this newly-started instance might not be consistent with the instance that has crashed. At this time, we must be careful enough to avoid causing possible issues.</p>
<p>Take a simple case as an example. The PD routing table is stored in etcd. However, the node itself determines the split of a Region. This way, the node can quickly know whether the size of one of its Regions exceeds the threshold. When this split event is actively pushed from the node to PD, if PD receives this event but crashes before persisting the state to etcd, the newly-started PD doesn't know about the split. At this point, the information in the routing table might be wrong.</p>
<p>What we do is design PD to be completely stateless. Only through making it completely stateless can we avoid various problems caused by failing to persist the state.</p>
<p>Periodically, each node sends information about the Regions on it to PD using heartbeats. Then, PD takes the information it receives and creates a global routing table. In this way, even if PD crashes, after the new PD starts, it only needs to wait for a few heartbeats and then it can get the global routing information again. In addition, PD can use etcd as a cache to accelerate this process. That is, after the new PD starts, it pulls the routing information from etcd, waits for a few heartbeats, and then provides services.</p>
<h3 id="the-epoch-mechanism-guarantees-the-latest-information">The epoch mechanism guarantees the latest information</h3>
<p>However, you might have noticed that there is still a problem. If the cluster has partitions in a certain section, the information about some nodes might be wrong.</p>
<p>For example, some Regions re-initiate elections and splits after they are split, but another isolated batch of nodes still sends the obsolete information to PD through heartbeats. So for one Region, either of two nodes might say that it's the leader, and the Region doesn't know whom to trust.</p>
<p>In TiKV, we use an epoch mechanism. With this mechanism, changes are marked with two logical clocks: one is the Raft's configuration change version, and the other is the Region version. For each configuration change, the configuration change version automatically increases. Similarly, for each Region change such as splitting or merging, the Region version automatically increases, too.</p>
<p>The previous section mentions that a split process with network isolation might result in complicated cases. The epoch mechanism we discuss here is the solution to the example case described above.</p>
<p>The epoch strategy that PD adopts is to get the larger value by comparing the logical clock values of two nodes. PD first compares values of the Region version of two nodes. If the values are the same, PD compares the values of the configuration change version. The node with a larger configuration change version must have the newer information.</p>
<h2 id="conclusion">Conclusion</h2>
<p>It's a highly complex project to build a robust distributed system. I've shared some of the key design ideas of building a large-scale distributed storage system based on the Raft consensus algorithm. If you're interested in how we implement TiKV, you're welcome to dive deep by reading our <a href="https://github.com/tikv/tikv">TiKV source code</a> and <a href="https://tikv.org/docs/">TiKV documentation</a>.</p>
<p><em>This article was first published on the <a href="https://www.cncf.io/blog/2019/11/04/building-a-large-scale-distributed-storage-system-based-on-raft/">CNCF website</a>.</em></p>
</div>
</div>
<div class="article-toc"></div><div class="ssba-wrap">
<div class="ssba">
<a class="ssba_twitter_share" data-site="" href="http://twitter.com/share?url=https%3a%2f%2fpingcap.com%2fblog%2fbuilding-a-large-scale-distributed-storage-system-based-on-raft%2f" target="_blank">
<img alt="Twitter icon" src="/images/svgs/twitter-icon.svg"/>
</a>
<a class="ssba_reddit_share" data-site="reddit" href="http://reddit.com/submit?url=https%3a%2f%2fpingcap.com%2fblog%2fbuilding-a-large-scale-distributed-storage-system-based-on-raft%2f&amp;title=Building%20a%20Large-scale%20Distributed%20Storage%20System%20Based%20on%20Raft" target="_blank">
<img alt="Reddit icon" src="/images/svgs/reddit-icon.svg"/>
</a>
<a class="ssba_facebook_share" data-site="" href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fpingcap.com%2fblog%2fbuilding-a-large-scale-distributed-storage-system-based-on-raft%2f" target="_blank">
<img alt="Fackbook icon" src="/images/svgs/facebook-icon.svg"/>
</a>
<a class="ssba_linkedin_share" data-site="linkedin" href="http://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fpingcap.com%2fblog%2fbuilding-a-large-scale-distributed-storage-system-based-on-raft%2f" target="_blank">
<img alt="Linkedin icon" src="/images/svgs/linkedin-icon.svg"/>
</a>
<a class="ssba_hackernews_share" data-site="hackernews" href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fpingcap.com%2fblog%2fbuilding-a-large-scale-distributed-storage-system-based-on-raft%2f&amp;t=Building%20a%20Large-scale%20Distributed%20Storage%20System%20Based%20on%20Raft" target="_blank">
<img alt="Hacker News icon" src="/images/svgs/hacker-news-icon.svg"/>
</a>
</div>
</div>
<div class="trackable-btns">
<a href="/download" onclick="trackViews('Building a Large-scale Distributed Storage System Based on Raft', 'download-tidb-btn-bottom')"><button>Download
                        TiDB</button></a>
<a href="https://share.hsforms.com/1e2W03wLJQQKPd1d9rCbj_Q2npzm" onclick="trackViews('Building a Large-scale Distributed Storage System Based on Raft', 'subscribe-blog-btn-bottom')"><button>Subscribe to
                        Blog</button></a>
</div>
</div>
<div class="sidebar-page">
<div class="sticky-sidebar">
<p class="toc-title">Whatâs on this page</p>
<div id="smart_toc_wrapper"></div>
</div>
</div>
</div>
</div>
<footer>
<div class="container">
<div class="flex-list-wrap">
<div class="flex-list">
<h4 class="list-title"><strong>Product</strong></h4>
<ul>
<li><a href="/docs/v3.0/">TiDB</a></li>
<li><a href="/docs/v3.0/reference/tispark/">TiSpark</a></li>
<li><a href="/docs/v3.0/roadmap/">Roadmap</a></li>
</ul>
</div>
<div class="flex-list">
<h4 class="list-title"><strong>Docs</strong></h4>
<ul>
<li><a href="/docs/stable/quick-start-with-tidb/">Quick Start</a></li>
<li><a href="/blog/2017-07-24-tidbbestpractice/">Best Practices</a></li>
<li><a href="/docs/stable/faq/tidb/">FAQ</a></li>
<li><a href="/docs/stable/reference/tools/user-guide/">TiDB Tools</a></li>
<li><a href="/docs/dev/releases/rn/">Release Notes</a></li>
</ul>
</div>
<div class="flex-list">
<h4 class="list-title"><strong>Resources</strong></h4>
<ul>
<li><a href="/blog/">Blog</a></li>
<li><a href="/weekly/">Monthly</a></li>
<li><a href="https://github.com/pingcap" target="_blank">GitHub</a></li>
<li><a href="/community">TiDB Community</a></li>
</ul>
</div>
<div class="flex-list">
<h4 class="list-title"><strong>Company</strong></h4>
<ul>
<li><a href="/about/">About</a></li>
<li><a href="https://angel.co/pingcap-1/jobs" target="_blank">Careers</a></li>
<li><a href="/news/">News</a></li>
<li><a href="/contact-us/">Contact Us</a></li>
<li><a href="/privacy-policy/">Privacy Policy</a></li>
<li><a href="/terms-of-service/">Terms of Service</a></li>
</ul>
</div>
</div>
<div class="contact-list-wrap">
<div class="flex-list">
<h4 class="list-title"><strong>Connect</strong></h4>
<ul>
<li><a class="twitter" href="https://twitter.com/PingCAP" target="_blank"><svg height="19" viewbox="0 0 612 612" width="18" xmlns="http://www.w3.org/2000/svg"><path d="M612 116.258a250.714 250.714 0 0 1-72.088 19.772c25.929-15.527 45.777-40.155 55.184-69.411-24.322 14.379-51.169 24.82-79.775 30.48-22.907-24.437-55.49-39.658-91.63-39.658-69.334 0-125.551 56.217-125.551 125.513 0 9.828 1.109 19.427 3.251 28.606-104.326-5.24-196.835-55.223-258.75-131.174-10.823 18.51-16.98 40.078-16.98 63.101 0 43.559 22.181 81.993 55.835 104.479a125.556 125.556 0 0 1-56.867-15.756v1.568c0 60.806 43.291 111.554 100.693 123.104-10.517 2.83-21.607 4.398-33.08 4.398-8.107 0-15.947-.803-23.634-2.333 15.985 49.907 62.336 86.199 117.253 87.194-42.947 33.654-97.099 53.655-155.916 53.655-10.134 0-20.116-.612-29.944-1.721 55.567 35.681 121.536 56.485 192.438 56.485 230.948 0 357.188-191.291 357.188-357.188l-.421-16.253c24.666-17.593 46.005-39.697 62.794-64.861z"></path></svg> Twitter</a></li>
<li><a class="linkedin" href="https://www.linkedin.com/company/pingcap/" target="_blank"><svg height="16" viewbox="0 0 438.536 438.535" width="18" xmlns="http://www.w3.org/2000/svg"><path d="M5.424 145.895H99.64v282.932H5.424zM408.842 171.739c-19.791-21.604-45.967-32.408-78.512-32.408-11.991 0-22.891 1.475-32.695 4.427-9.801 2.95-18.079 7.089-24.838 12.419-6.755 5.33-12.135 10.278-16.129 14.844-3.798 4.337-7.512 9.389-11.136 15.104v-40.232h-93.935l.288 13.706c.193 9.139.288 37.307.288 84.508 0 47.205-.19 108.777-.572 184.722h93.931V270.942c0-9.705 1.041-17.412 3.139-23.127 4-9.712 10.037-17.843 18.131-24.407 8.093-6.572 18.13-9.855 30.125-9.855 16.364 0 28.407 5.662 36.117 16.987 7.707 11.324 11.561 26.98 11.561 46.966V428.82h93.931V266.664c-.007-41.688-9.897-73.328-29.694-94.925zM53.103 9.708c-15.796 0-28.595 4.619-38.4 13.848C4.899 32.787 0 44.441 0 58.529 0 72.42 4.758 84.034 14.275 93.358c9.514 9.325 22.078 13.99 37.685 13.99h.571c15.99 0 28.887-4.661 38.688-13.99 9.801-9.324 14.606-20.934 14.417-34.829-.19-14.087-5.047-25.742-14.561-34.973C81.562 14.323 68.9 9.708 53.103 9.708z"></path></svg> LinkedIn</a></li>
<li><a class="reddit" href="https://www.reddit.com/r/TiDB/" target="_blank"><svg height="18" viewbox="0 0 279.748 279.748" width="18" xmlns="http://www.w3.org/2000/svg"><path d="M279.748 133.142c0-19.299-15.701-35-35-35-10.768 0-20.674 4.812-27.279 13.064-18.532-8.431-39.663-13.626-62.015-15.271l19.206-60.692 42.895 9.294c3.285 12.782 14.901 22.258 28.693 22.258 16.336 0 29.627-13.29 29.627-29.626 0-16.336-13.291-29.627-29.627-29.627-11.801 0-21.999 6.941-26.759 16.95l-49.497-10.725a10.002 10.002 0 0 0-11.651 6.756L134.636 95.43c-26.164.638-50.988 6.053-72.356 15.775-6.606-8.251-16.512-13.063-27.28-13.063-19.299 0-35 15.701-35 35 0 9.373 3.683 18.173 10.222 24.709-3.9 8.37-5.875 17.076-5.875 25.936 0 24.048 14.396 46.492 40.538 63.199 25.447 16.264 59.183 25.221 94.989 25.221 35.808 0 69.542-8.957 94.989-25.221 26.142-16.707 40.538-39.151 40.538-63.199 0-8.859-1.975-17.565-5.875-25.936 6.539-6.537 10.222-15.336 10.222-24.709zM15.369 145.139c-2.212-3.59-3.369-7.688-3.369-11.997 0-12.682 10.317-23 23-23 5.444 0 10.558 1.851 14.649 5.258-14.622 8.302-26.132 18.289-34.28 29.739zm52.671 20.266c0-13.785 11.215-25 25-25s25 11.215 25 25-11.215 25-25 25-25-11.215-25-25zm123.119 57.054c-9.745 10.637-29.396 17.244-51.285 17.244-21.888 0-41.539-6.607-51.284-17.244a9.937 9.937 0 0 1-2.617-7.192 9.933 9.933 0 0 1 3.235-6.937 9.974 9.974 0 0 1 6.754-2.627c2.797 0 5.484 1.183 7.373 3.244 5.803 6.333 20.827 10.756 36.539 10.756s30.737-4.423 36.539-10.756a10.022 10.022 0 0 1 7.374-3.244c2.508 0 4.906.933 6.755 2.627a9.928 9.928 0 0 1 3.234 6.937 9.933 9.933 0 0 1-2.617 7.192zm-4.451-32.054c-13.785 0-25-11.215-25-25s11.215-25 25-25 25 11.215 25 25-11.215 25-25 25zm77.671-45.266c-8.147-11.45-19.657-21.436-34.28-29.739 4.092-3.408 9.205-5.258 14.649-5.258 12.683 0 23 10.318 23 23 0 4.309-1.157 8.407-3.369 11.997z"></path></svg>Reddit</a></li>
<li><a class="google-plus" href="https://groups.google.com/forum/#!forum/tidb-user" target="_blank"><svg height="20" viewbox="0 0 491.858 491.858" width="18" xmlns="http://www.w3.org/2000/svg"><path d="M377.472 224.957H201.319v58.718H308.79c-16.032 51.048-63.714 88.077-120.055 88.077-69.492 0-125.823-56.335-125.823-125.824 0-69.492 56.333-125.823 125.823-125.823 34.994 0 66.645 14.289 89.452 37.346l42.622-46.328c-34.04-33.355-80.65-53.929-132.074-53.929C84.5 57.193 0 141.693 0 245.928s84.5 188.737 188.736 188.737c91.307 0 171.248-64.844 188.737-150.989v-58.718l-.001-.001zM491.858 224.857h-36.031v-36.031h-30.886v36.031H388.91v30.883h36.031v36.032h30.886V255.74h36.031z"></path></svg>Google Group</a></li>
<li><a class="stack-overflow" href="https://stackoverflow.com/questions/tagged/tidb" target="_blank"><svg height="17" viewbox="0 0 547.597 547.597" width="18" xmlns="http://www.w3.org/2000/svg"><path d="M140.81 475.111h.024l189.91-.269c8.434-.013 15.3-6.886 15.3-15.318v-21.298c0-8.428-6.854-15.288-15.3-15.288l-189.91.264c-8.433.012-15.3 6.885-15.3 15.318v21.31c.001 8.427 6.855 15.281 15.276 15.281z"></path><path d="M58.667 517.854c.073 29.26.073 29.449 3.072 29.449h.055l10.612.294h.086s85.814 0 171.629-.036c42.914-.019 85.821-.05 118-.092 16.09-.019 29.505-.043 38.887-.074 17.803-.055 17.803-.055 17.803-3.072l.3-10.697V333.299c0-8.434-6.866-15.3-15.3-15.3h-11.909c-8.434 0-15.3 6.866-15.3 15.3V496.22c0 5.062-4.119 9.18-9.181 9.18H110.51c-5.061 0-9.18-4.118-9.18-9.18V333.299c0-8.434-6.867-15.3-15.3-15.3H73.82c-8.433 0-15.3 6.86-15.3 15.3 0 23.342.012 76.084.055 122.981.019 23.447.05 45.442.092 61.574z"></path><path d="M142.322 397.399l189.402 17.467c.478.043.948.062 1.413.062 7.956 0 14.468-5.985 15.159-13.917l1.83-21.096c.729-8.396-5.508-15.851-13.898-16.628l-189.102-17.461c-8.593-.795-15.869 5.441-16.653 13.825l-1.97 21.108a15.172 15.172 0 0 0 3.452 11.181 15.19 15.19 0 0 0 10.367 5.459zM437.636 208.849a15.253 15.253 0 0 0 17.694 12.443l21.065-3.678c8.311-1.451 13.898-9.395 12.454-17.706l-32.504-187.23c-1.42-8.207-9.21-13.917-17.692-12.448l-21.065 3.678c-8.311 1.451-13.898 9.395-12.454 17.705l32.502 187.236zM190.333 194.375l163.6 96.708a15.28 15.28 0 0 0 7.778 2.136c5.393 0 10.447-2.876 13.183-7.509l10.876-18.36c2.08-3.519 2.674-7.631 1.658-11.591a15.18 15.18 0 0 0-7.032-9.358l-163.6-96.714c-7.014-4.149-16.836-1.597-20.967 5.374l-10.869 18.36a15.2 15.2 0 0 0-1.658 11.591 15.21 15.21 0 0 0 7.031 9.363zM387.525 240.501a15.276 15.276 0 0 0 12.626 6.659c3.091 0 6.077-.924 8.635-2.681l17.405-11.934c6.953-4.768 8.752-14.314 4.015-21.292L323.29 54.104c-4.584-6.732-14.498-8.623-21.236-3.984l-17.737 12.21c-6.945 4.779-8.727 14.327-3.971 21.291l107.179 156.88zM154.482 302.319l183.465 49.156c1.298.349 2.632.526 3.966.526 6.903 0 12.98-4.67 14.762-11.347l5.508-20.624c2.179-8.152-2.681-16.555-10.826-18.74l-183.465-49.162c-8.017-2.148-16.604 2.852-18.728 10.826l-5.508 20.63c-2.179 8.142 2.68 16.551 10.826 18.735z"></path></svg>Stack Overflow</a></li>
</ul>
</div>
<div class="subscribe">
<a href="https://share.hsforms.com/1e2W03wLJQQKPd1d9rCbj_Q2npzm"><button class="btn btn-subscribe f-tc">Subscribe to Blog</button></a>
</div>
</div>
</div>
<div class="container copyright-container">
<p class="copyright">Â© 2020 PingCAP. All Rights Reserved.</p>
<a class="copyright-btn-lang" href="/blog-cn" id="lang">ä¸­æ</a>
</div>
</footer>
<button class="back-to-top" type="button"><svg height="512" viewbox="0 0 284.929 284.929" width="512" xmlns="http://www.w3.org/2000/svg"><path d="M282.082 195.285L149.028 62.24c-1.901-1.903-4.088-2.856-6.562-2.856s-4.665.953-6.567 2.856L2.856 195.285C.95 197.191 0 199.378 0 201.853c0 2.474.953 4.664 2.856 6.566l14.272 14.271c1.903 1.903 4.093 2.854 6.567 2.854s4.664-.951 6.567-2.854l112.204-112.202 112.208 112.209c1.902 1.903 4.093 2.848 6.563 2.848 2.478 0 4.668-.951 6.57-2.848l14.274-14.277c1.902-1.902 2.847-4.093 2.847-6.566.001-2.476-.944-4.666-2.846-6.569z" fill="#FFF"></path></svg>
</button>
</div><div class="overlay"></div><script src="https://download.pingcap.com/js/jquery.min.js"></script><script src="/js/vendor/lazyload.min.js" type="text/javascript"></script>
<script src="/js/doc.js" type="text/javascript"></script>
<script src="/js/anchor.js" type="text/javascript"></script><script src="https://cdn.jsdelivr.net/algoliasearch/3/algoliasearch.min.js"></script><script src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script><script crossorigin="anonymous" integrity="sha384-jbFinqIbKkHNg+QL+yxB4VrBC0EAPTuaLGeRT0T+NfEV89YC6u1bKxHLwoo+/xxY" src="https://browser.sentry-cdn.com/5.11.0/bundle.min.js"></script><script>Sentry.init({ 
            dsn: 'https://3f28ed393c5545daa74496b3cdb2e9ba@sentry.io/1887163' 
        });</script></body></html>